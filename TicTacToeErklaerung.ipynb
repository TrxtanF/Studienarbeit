{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning mit Tic Tac Toe\n",
    "\n",
    "In diesem Notebook werden wir 2 Agenten in Tic Tac Toe trainieren, indem sie gegeneinander Spielen und voneinander lernen. Das erlernte Spiel wird in einer Policy gespeichert. \n",
    "Anschließend kann diese Policy genutzt werden um die Agenten auch gegen Menschen spielen zu lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was ist Pickle?\n",
    "\n",
    "Pickle wird als Python library verwendet um am Ende unsere Policy darin zu speichern. Ohne Pickle müsste der Agent jedes mal erneut Trainiert werden, wenn das Programm gestartet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reinforcement Learning](RL.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Tic Tac Toe umzusetzen benötigen wir erstmal unser Environment, einen Agenten, Aktionen die der Agent in dem Environment ausführen kann und Rewards für die Aktionen. \n",
    "\n",
    "Unser Environment besteht einfach aus einem 3x3 Raster. Es stellt die Rahmenbedingungen dar, in denen der Agent agiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden 2 Agenten nutzen, welche Gegeneinander spielen. Also wird eine Spielerklasse erstellt. Für das Spielen der Agenten wird eine Zustandsklasse erstellt, in welcher die Züge und rewards definiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beginnen mit der Init Funktion für das Spielfeld. Wir erstellen ein leeres Board mit zwei Spielern *p1* und *p2*. Beide Erhalten ein Spielersymbol, welches auf dem Board gesetzt wird, sobald einer der Spieler eine Aktion ausführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # init p1 plays first\n",
    "        self.playerSymbol = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die **getHash** Funktion hasht den aktuellen Zustand des Spielbretts, sodass es in einer State-value Bibliothek gespeichert werden kann. Diese State-Value Bibliothek spielt eine wichtige Rolle in der Entscheidungsfindung der Agenten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHash(self):\n",
    "    self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "    return self.boardHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobald ein Spieler eine Aktion ausgeführt hat, wird das Spielersymbol auf dem Spielbrett eingetragen und die Verfügbaren Positionen geupdated. Außerdem wird der Spieler gewechselt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "\n",
    "def updateState(self, position):\n",
    "    self.board[position] = self.playerSymbol\n",
    "    # switch to another player\n",
    "    self.playerSymbol = -1 if self.playerSymbol == 1 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach jeder Aktion wird geprüft ob bereits ein Gewinner feststeht. Falls ein Gewinner feststeht muss außerdem ein Reward verteilt werden. Die Funktion prüft zuerst ob in einer Reihe oder Zeile 3 Spieler Symbole liegen und anschließend in der Diagonalen. \n",
    "\n",
    "Bei Beendung des Spiels werden folgende Rewards verteilt:\n",
    "\n",
    "- 1, wenn gewonnen\n",
    "- 0, wenn verloren\n",
    "- 0.1 für *p1*, wenn Unentschieden (Unentschieden soll vermieden werden) \n",
    "- 0.5 für *p2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winner(self):\n",
    "        # row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(abs(diag_sum1), abs(diag_sum2))\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            if diag_sum1 == 3 or diag_sum2 == 3:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "\n",
    "# only when game ends\n",
    "def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(0)\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(1)\n",
    "        else:\n",
    "            self.p1.feedReward(0.1)\n",
    "            self.p2.feedReward(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spielerklasse\n",
    "\n",
    "Wir benötigen nun eine Spielerklasse die unseren Agenten darstellt. Der Agent muss für folgendes in der Lage sein:\n",
    "\n",
    "1. Aktionen auswählen basierend auf den bekannten Zuständen \n",
    "2. Alle Zustände des Spiels aufzeichnen \n",
    "3. Seine States-Value Schätzung nach jedem Spiel updaten\n",
    "4. Seine Policy speichern und laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Liste *self.states* halten wir alle Züge fest, die während des Spiels gemacht wurden.  \n",
    "Wir legen ein *dict* für die State-Value paare an und updaten die Schätzung am Ende jedes Spiels.  \n",
    "Die Learning Rate (self.lr) bestimmt, wie stark die Gewichtung von neuen Informationen gegüber bereits vorhandenen Informationen ist, wenn der Reward für einen bestimmten Zustand Aktuallisiert wird. Die LR spielt am ende jedes Spiels eine Rolle, wenn wir unsere State-Values Aktualisieren.  \n",
    "Wir nutzen den ϵ-greedy um eine Balancierung zwischen Exploration und Exploitation zu nutzen (0.3). Das Bedeutet 70% der Zeit sind wir greedy und wählen unsere Aktion basierend auf unserer aktuellen Schätzung aus und 30% der Zeit wählen wir einen zufälligen Schritt.  \n",
    "Das Decay Gamma bestimmt, wie wichtig zukünftige Belohnungen im Vergleich zu unmittelbaren Belohnungen sind. Bei einer 1 würden wir zukünftige Belohnungen genauso hoch gewichten wie unmittelbare Belohnungen, was einer langfristige Sichweise entspricht. Bei 0 würden wir nur unmittelbare Belohnungen berücksichtigen (kurzfristige Sichtweise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, name, exp_rate=0.3):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # state -> value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wird die **chooseAction** Methode implementiert. Hier nutzen wir unseren Greedy Faktor um zuerst zu bestimmen ob wir eine zufällige Position wählen oder unser State-value dict nutzen. Falls wir unser State-Value dict nutzen, erstellen wir nun für alle Möglichen positionen ein neues Board setzen unser Spielersymbol. So können wir einmal durch alle Möglichen Positionen durch iterieren und wählen den Zug, mit der dem höchsten State-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](State-value.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kommt der wichtigste Teil. Die Rückpropagierung. Für die Rückpropagierung wird folgende Formel verwendet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Formel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Formel muss nicht genau verstanden werden, wichtig ist nur, dass es sich um sogenannte Werteiteration handelt. Die Logik dahinter ist, dass wir den aktuellen Wert langsam auf Basis unserer neuesten Beobachtung anpassen. Alle unsere Positionen und Züge werden während des Spiels in der Liste self.States gespeichert. Wenn der Agent das Ende des Spiels erreicht, werden die Schätzwerte in umgekehrter Reihenfolge aktualisiert. So ergeben sich die State-Values und der Agent wird besser im Spiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Nun ist der Agent in der Lage zu lernen und alles ist aufgesetzt. Wir lassen nun beide Spieler gegeneinander Spielen um gegenseitig voneinander zu lernen.  \n",
    "  \n",
    "Während des Trainings durchgeht jeder Agent für jeden Zug folgenden Prozess:\n",
    "- Prüft welche offenen Positionen es gibt\n",
    "- Wählt eine Aktion/Handlung aus\n",
    "- Updatet den Zustand des Spielbretts und fügt seinen Spielzug der Liste self.states hinzu\n",
    "- Prüft ob das Spiel vorbei ist und verteilt dementsprechend rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(self, rounds=100):\n",
    "        for i in range(rounds):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.availablePositions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                # take action and upate board state\n",
    "                self.updateState(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.availablePositions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "\n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Speichern\n",
    "\n",
    "Am Ende muss nur noch die Policy gespeichert werden, sodass wir diese Laden können um gegen einen Menschen zu spielen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePolicy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "def loadPolicy(self, file):\n",
    "        fr = open(file, 'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mensch gegen Computer\n",
    "\n",
    "Wir erstellen einen Menschklasse, welche dann gegen unsere Agenten spielen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
