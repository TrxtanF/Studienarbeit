{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Trading mit DQN: Training & Testen\n",
    "\n",
    "Dieses Notebook demonstriert ein Bitcoin Trading Environment, in dem ein DQN-Agent auf historischen Daten trainiert wird und anschließend eine Trading-Strategie entwickelt. Dabei werden:\n",
    "\n",
    "- **Training:** Daten aus dem Zeitraum 2015 bis 2020 verwendet.\n",
    "- **Test:** Daten aus dem Zeitraum 2021 bis 2025 genutzt.\n",
    "\n",
    "Das Portfolio startet als Multiplikator bei 1.0 (d. h. ein Wert von 1.2 entspricht +20% Gewinn, 0.8 entspricht -20% Verlust). Außerdem wird der Trading-Verlauf gegen eine Buy-and-Hold-Strategie in einem logarithmisch skalierten Diagramm (mit Datumsangaben) verglichen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from gym import spaces\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment-Definition\n",
    "\n",
    "Das Environment `BitcoinTradingEnv`:\n",
    "- Lädt historische Bitcoin-Daten und filtert diese anhand eines übergebenen Datumsbereichs.\n",
    "- Nutzt ein Fenster (window_size) der skalierten Daten als Beobachtung, ergänzt durch den Positionsstatus und den aktuellen Portfolio-Multiplikator.\n",
    "- Berechnet den Reward basierend auf der Änderung des Portfolio-Multiplikators.\n",
    "- Beim Rendern und Plotten wird das aktuelle Datum angezeigt.\n",
    "\n",
    "Im Plot wird der Verlauf des Trading-Portfolios mit einer Buy-and-Hold-Strategie (logarithmische y-Achse) verglichen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, window_size=30, initial_capital=1.0,\n",
    "                 overall_start_date='2014-09-17', overall_end_date='2025-12-31',\n",
    "                 date_range=None):\n",
    "        super(BitcoinTradingEnv, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.initial_capital = initial_capital\n",
    "        \n",
    "        # Lade historische Bitcoin-Daten\n",
    "        self.df = yf.download('BTC-USD', start=overall_start_date, end=overall_end_date)\n",
    "        self.df.columns = self.df.columns.get_level_values(0)\n",
    "        # Filtere Daten anhand des angegebenen Datumsbereichs (z. B. Training oder Test)\n",
    "        if date_range is not None:\n",
    "            self.df = self.df.loc[date_range[0]:date_range[1]]\n",
    "            \n",
    "        self.data = self.df[['Close', 'High', 'Low', 'Open', 'Volume']].copy()\n",
    "        self.num_features = self.data.shape[1]\n",
    "        \n",
    "        # Skaliere die Features in den Bereich [0, 1]\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.scaled_features = self.scaler.fit_transform(self.data.values)\n",
    "        self.close_prices = self.df['Close'].values\n",
    "        \n",
    "        # Aktionsraum: 0 = Halten, 1 = Kaufen, 2 = Verkaufen\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        # Beobachtungsraum: window, position und portfolio (Multiplikator)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'window': spaces.Box(low=0, high=1, shape=(self.window_size, self.num_features), dtype=np.float32),\n",
    "            'position': spaces.Discrete(2),\n",
    "            'portfolio': spaces.Box(low=0, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size  # Beginne, wenn ein vollständiges Fenster vorliegt\n",
    "        self.portfolio = self.initial_capital    # Startwert = 1.0\n",
    "        self.position = 0  # 0 = nicht investiert, 1 = investiert\n",
    "        self.buy_price = None\n",
    "        self.portfolio_history = [self.portfolio]\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return {\n",
    "            'window': self.scaled_features[self.current_step - self.window_size : self.current_step].astype(np.float32),\n",
    "            'position': self.position,\n",
    "            'portfolio': np.array([self.portfolio], dtype=np.float32)\n",
    "        }\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        penalty = 10\n",
    "        \n",
    "        # Wenn das Ende der Daten erreicht ist\n",
    "        if self.current_step >= len(self.scaled_features) - 1:\n",
    "            done = True\n",
    "            return self._get_observation(), reward, done, {}\n",
    "        \n",
    "        prev_price = self.close_prices[self.current_step - 1]\n",
    "        current_price = self.close_prices[self.current_step]\n",
    "        prev_portfolio = self.portfolio\n",
    "        \n",
    "        if self.position == 1:\n",
    "            self.portfolio *= (current_price / prev_price)\n",
    "            \n",
    "        if action == 1:  # Kaufen\n",
    "            if self.position == 0:\n",
    "                self.position = 1\n",
    "                self.buy_price = current_price\n",
    "            else:\n",
    "                reward -= penalty\n",
    "        elif action == 2:  # Verkaufen\n",
    "            if self.position == 1:\n",
    "                self.position = 0\n",
    "                self.buy_price = None\n",
    "            else:\n",
    "                reward -= penalty\n",
    "        elif action == 0:  # Halten\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Ungültige Aktion\")\n",
    "        \n",
    "        reward += self.portfolio - prev_portfolio\n",
    "        \n",
    "        self.current_step += 1\n",
    "        self.portfolio_history.append(self.portfolio)\n",
    "        if self.current_step >= len(self.scaled_features) - 1:\n",
    "            done = True\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        date = self.df.index[self.current_step].strftime(\"%Y-%m-%d\")\n",
    "        status = \"Invested\" if self.position == 1 else \"Not invested\"\n",
    "        current_price = self.close_prices[self.current_step]\n",
    "        print(f\"Date: {date} | Portfolio: {self.portfolio:.4f} | Position: {status} | Price: {current_price:.2f}\")\n",
    "    \n",
    "    def plot_comparison_log(self):\n",
    "        # Buy-and-Hold: ab dem Start der Episode investiert und bis zum Ende gehalten\n",
    "        initial_index = self.window_size  \n",
    "        initial_price = self.close_prices[initial_index]\n",
    "        buy_and_hold = [\n",
    "            self.initial_capital * (self.close_prices[initial_index + i] / initial_price)\n",
    "            for i in range(len(self.portfolio_history))\n",
    "        ]\n",
    "        dates = self.df.index[self.window_size:self.window_size + len(self.portfolio_history)]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(dates, self.portfolio_history, label=\"Trading Portfolio\")\n",
    "        plt.plot(dates, buy_and_hold, label=\"Buy and Hold Portfolio\", linestyle=\"--\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Portfolio Multiplier\")\n",
    "        plt.title(\"Trading vs. Buy and Hold (Log Scale)\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.gca().yaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN-Agent und Replay Buffer\n",
    "\n",
    "Der Agent besteht aus einem einfachen Feedforward-Netzwerk (`QNetwork`), das die Q-Werte approximiert.  \n",
    "Mit einem Replay Buffer werden Erfahrungen gespeichert und in Batches zum Training verwendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion: Zustand (Dictionary) flach zu einem Vektor transformieren\n",
    "def flatten_state(state):\n",
    "    window = state['window'].flatten()\n",
    "    position = np.array([state['position']], dtype=np.float32)\n",
    "    portfolio = state['portfolio']\n",
    "    return np.concatenate([window, position, portfolio])\n",
    "\n",
    "# Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training und Testen\n",
    "\n",
    "- **Training:**  \n",
    "  Der Agent wird auf den Daten von 2015 bis 2020 trainiert.\n",
    "  \n",
    "- **Test:**  \n",
    "  Anschließend wird der Agent auf den Daten von 2021 bis 2025 eingesetzt.  \n",
    "  Zum Schluss wird der Verlauf des Trading-Portfolios (gegenüber Buy-and-Hold) in einem logarithmischen Diagramm mit Datum auf der x-Achse geplottet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Hyperparameter\n",
    "    num_episodes = 50\n",
    "    batch_size = 64\n",
    "    gamma = 0.99\n",
    "    learning_rate = 1e-3\n",
    "    buffer_capacity = 10000\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    target_update_freq = 5  # Update Target-Netzwerk alle 5 Episoden\n",
    "    \n",
    "    window_size = 30\n",
    "    # Training: 2015-01-01 bis 2020-12-31\n",
    "    train_env = BitcoinTradingEnv(window_size=window_size, initial_capital=1.0,\n",
    "                                  overall_start_date='2014-09-17', overall_end_date='2025-12-31',\n",
    "                                  date_range=(\"2015-01-01\", \"2020-12-31\"))\n",
    "    \n",
    "    sample_obs = train_env.reset()\n",
    "    flat_sample = flatten_state(sample_obs)\n",
    "    input_dim = flat_sample.shape[0]\n",
    "    output_dim = train_env.action_space.n\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    q_net = QNetwork(input_dim, output_dim).to(device)\n",
    "    target_net = QNetwork(input_dim, output_dim).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    print(\"Training started...\")\n",
    "    for episode in range(num_episodes):\n",
    "        state = train_env.reset()\n",
    "        state_flat = flatten_state(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Epsilon-greedy Aktion\n",
    "            if random.random() < epsilon:\n",
    "                action = train_env.action_space.sample()\n",
    "            else:\n",
    "                state_tensor = torch.FloatTensor(state_flat).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = q_net(state_tensor)\n",
    "                action = int(torch.argmax(q_values, dim=1).item())\n",
    "            next_state, reward, done, _ = train_env.step(action)\n",
    "            next_state_flat = flatten_state(next_state)\n",
    "            replay_buffer.push(state_flat, action, reward, next_state_flat, done)\n",
    "            state_flat = next_state_flat\n",
    "            total_reward += reward\n",
    "            \n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states = torch.FloatTensor(states).to(device)\n",
    "                actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "                rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "                next_states = torch.FloatTensor(next_states).to(device)\n",
    "                dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "                \n",
    "                q_values = q_net(states).gather(1, actions)\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "                    target = rewards + gamma * next_q_values * (1 - dones)\n",
    "                loss = nn.MSELoss()(q_values, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        print(f\"Episode {episode+1}/{num_episodes} - Total Reward: {total_reward:.4f} - Epsilon: {epsilon:.4f}\")\n",
    "        if (episode+1) % target_update_freq == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "    print(\"Training completed.\")\n",
    "    \n",
    "    # Testphase: 2021-01-01 bis 2025-12-31\n",
    "    test_env = BitcoinTradingEnv(window_size=window_size, initial_capital=1.0,\n",
    "                                 overall_start_date='2014-09-17', overall_end_date='2025-12-31',\n",
    "                                 date_range=(\"2021-01-01\", \"2025-12-31\"))\n",
    "    \n",
    "    state = test_env.reset()\n",
    "    state_flat = flatten_state(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state_flat).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state_tensor)\n",
    "        action = int(torch.argmax(q_values, dim=1).item())\n",
    "        next_state, reward, done, _ = test_env.step(action)\n",
    "        state_flat = flatten_state(next_state)\n",
    "    \n",
    "    test_env.plot_comparison_log()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
