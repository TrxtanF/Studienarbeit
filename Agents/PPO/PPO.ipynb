{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-VD1817E\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Liste der ben√∂tigten Bibliotheken\n",
    "required_packages = [\n",
    "    \"gymnasium\", \"numpy\", \"pandas\", \"joblib\", \"scikit-learn\", \"matplotlib\",\n",
    "    \"stable-baselines3\", \"torch\", \"torchvision\", \"torchaudio\"\n",
    "]\n",
    "\n",
    "# Funktion zum Installieren fehlender Bibliotheken\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            print(f\"üì¶ Installiere {package} ...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Installiere fehlende Pakete\n",
    "install_packages(required_packages)\n",
    "\n",
    "# üîπ Separate Installation f√ºr sb3_contrib\n",
    "try:\n",
    "    import sb3_contrib\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installiere sb3_contrib ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"stable-baselines3[extra]\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sb3-contrib\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Studienarbeit_GitHub\\Studienarbeit\\Agents\\PPO\n",
      "Notebook ausgef√ºhrt\n"
     ]
    }
   ],
   "source": [
    "#%run /home/dhbw/environment.ipynb\n",
    "%run ../../Environment/environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import random\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "SEED  = seed % (2**32 - 1)\n",
    "print(f\"SEED: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich eingelesen\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CSV Datem einlesen\n",
    "# -------------------------------\n",
    "dhbw_train_path = \"/home/dhbw/2023-2018_stand_data.csv\"\n",
    "desktop_train_path = \"../../Transform_data/stand_data/2023-2018_stand_data.csv\"\n",
    "train_data = pd.read_csv(desktop_train_path)\n",
    "train_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "dhbw_test_path = \"/home/dhbw/2025-2024_stand_data.csv\"\n",
    "desktop_test_path = \"../../Transform_data/stand_data/2025-2024_stand_data.csv\"\n",
    "test_data = pd.read_csv(desktop_test_path)\n",
    "test_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "if(train_data is not None and test_data is not None):\n",
    "    print(\"Daten erfolgreich eingelesen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "def make_env():\n",
    "    return TradingEnv(\n",
    "        data=train_data,\n",
    "        initial_cash=10_000,\n",
    "        window_size=336,\n",
    "        scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "        default_seed=SEED\n",
    "    )\n",
    "\n",
    "n_envs = 8  # Mehr parallele Umgebungen (8, 16 oder sogar 32 testen!)\n",
    "env = SubprocVecEnv([make_env for _ in range(n_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Umgebung f√ºr Evaluation (ohne SubprocVecEnv, da wir nur eine Instanz brauchen)\n",
    "test_env = TradingEnv(\n",
    "    data=test_data,  # Oder test_data, falls du separate Test-Daten hast\n",
    "    initial_cash=10_000,\n",
    "    window_size=336,\n",
    "    scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "env.training = True  # Sicherstellen, dass Normalisierung aktiv ist\n",
    "env.device = \"cuda\"  # WICHTIG: Umgebung auf CUDA setzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === Test ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed in the environment: 42\n"
     ]
    }
   ],
   "source": [
    "dhbw_scaler = \"/home/dhbw/scaler.pkl\"\n",
    "desktop_scaler = \"../../Transform_data/scaler.pkl\"\n",
    "\n",
    "env = TradingEnv(\n",
    "        data=train_data,\n",
    "        initial_cash=10_000,\n",
    "        window_size=336,\n",
    "        scaler_path=desktop_scaler,\n",
    "        default_seed=SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed in the environment: 42\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TradingEnv' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTradingEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_cash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m336\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesktop_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m MaskablePPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, use_masking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\tlfin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:31\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[1;34m(self, env_fns)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: \u001b[38;5;28mlist\u001b[39m[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_patch_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_fns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\tlfin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: \u001b[38;5;28mlist\u001b[39m[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mTradingEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_cash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m336\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesktop_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m MaskablePPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, use_masking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TradingEnv' object is not callable"
     ]
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda: TradingEnv(\n",
    "        data=train_data,\n",
    "        initial_cash=10_000,\n",
    "        window_size=336,\n",
    "        scaler_path=desktop_scaler,\n",
    "        default_seed=SEED\n",
    "    )()])\n",
    "model = MaskablePPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100000, use_masking=True)\n",
    "\n",
    "# üöÄ Prediction auf ungesehenen Daten\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action_masks = env.get_action_mask()  # üî• Maske f√ºr g√ºltige Aktionen holen\n",
    "    action, _ = model.predict(obs, action_masks=action_masks)  # üéØ Maske nutzen\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed in the environment: 42\n",
      "üöÄ Unterst√ºtzt Masking?: False\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.utils import is_masking_supported\n",
    "\n",
    "env = TradingEnv(\n",
    "        data=train_data,\n",
    "        initial_cash=10_000,\n",
    "        window_size=336,\n",
    "        scaler_path=desktop_scaler,\n",
    "        default_seed=SEED\n",
    "    )\n",
    "\n",
    "print(\"üöÄ Unterst√ºtzt Masking?:\", is_masking_supported(env))  # Sollte `True` sein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === Ende ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  # F√ºr die Netzwerkarchitektur\n",
    "\n",
    "# Definiere das neuronale Netz\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[512, 512, 128], vf=[512, 512, 128])],  # Zwei gro√üe Layer (512) und ein kleinerer Layer (128)\n",
    "    activation_fn=nn.ReLU,\n",
    ")\n",
    "\n",
    "# Erstelle den PPO-Agenten mit verbesserten Einstellungen\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.0001,  # Lernrate\n",
    "    gamma=0.99,  # Discount-Faktor\n",
    "    clip_range=0.2,  # PPO-Clip-Parameter\n",
    "    ent_coef=0.01,  # Entropie-Koeffizient\n",
    "    n_steps=4_096,  # WICHTIG: Mehr Schritte pro Update ‚Üí GPU-Auslastung steigt\n",
    "    batch_size=4_096,  # WICHTIG: Gro√üe Batch-Gr√∂√üe ‚Üí GPU rechnet effizienter\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    device=\"cuda\",  # Nutzt die GPU!\n",
    "    #tensorboard_log=\"./tensorboard_log/\"  # Optional: Logging f√ºr TensorBoard\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Env l√§uft auf: {env.device}\")  # Sollte CUDA anzeigen\n",
    "print(\"Modell l√§uft auf:\", model.device)\n",
    "print(\"Model Policy:\", model.policy.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.policy))\n",
    "print(hasattr(model.policy, 'to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.to(\"cuda\")\n",
    "print(\"Policy erfolgreich auf GPU gesetzt:\", next(model.policy.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Optional: Checkpoint Callback, falls du Zwischenspeicherungen m√∂chtest\n",
    "# checkpoint_callback = CheckpointCallback(save_freq=100, save_path='./logs/', name_prefix='ppo_trading')\n",
    "\n",
    "# -------------------------------\n",
    "# Trainings- und Test-Performance (Loss) evaluieren und plotten\n",
    "# -------------------------------\n",
    "# Wir unterteilen das Training in mehrere Intervalle.\n",
    "eval_interval = 8_192            # Trainingsschritte pro Intervall\n",
    "total_timesteps = 200_000       # Gesamtzahl der Trainingsschritte\n",
    "n_iterations = total_timesteps // eval_interval\n",
    "\n",
    "# Listen f√ºr Plot-Daten\n",
    "train_loss_list = []  # Wir definieren Loss als negativen Reward (damit \"kleiner\" besser ist)\n",
    "test_loss_list = []\n",
    "timesteps_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamps = 250_000\n",
    "print(\"Modell l√§uft auf:\", model.device)\n",
    "\n",
    "# Debug-Print, um den Typ von model.policy zu pr√ºfen\n",
    "print(f\"Typ von model.policy: {type(model.policy)}\")\n",
    "\n",
    "# Modell auf GPU setzen, ohne model.policy zu ersetzen\n",
    "model.policy.to(\"cuda\")\n",
    "# Nur die forward()-Methode kompilieren, nicht das gesamte Policy-Objekt √ºberschreiben\n",
    "model.policy.forward = torch.compile(model.policy.forward)\n",
    "\n",
    "# Pr√ºfen, ob das Modell wirklich auf CUDA ist (Policy-Parameter verwenden)\n",
    "print(\"Modell auf Ger√§t:\", next(model.policy.parameters()).device)\n",
    "\n",
    "# Teste, ob Stable-Baselines3 wirklich Tensoren auf der GPU erstellt\n",
    "test_tensor = torch.randn(10, 10).to(\"cuda\")\n",
    "print(\"GPU-Test-Tensor erstellt:\", test_tensor.device)\n",
    "\n",
    "# Training um 'timestamps' Timesteps\n",
    "print(\"Training beginnt.\")\n",
    "start_time = time.time()\n",
    "\n",
    "#with torch.amp.autocast(\"cuda\"):\n",
    "model.learn(total_timesteps=timestamps, reset_num_timesteps=False)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "duration_minutes = duration / 60.0\n",
    "print(f\"Training abgeschlossen. {timestamps} Timesteps haben {duration:.2f} Sekunden ({duration_minutes:.2f} Minuten) gedauert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest-Train-Umgebung f√ºr Evaluation (ohne SubprocVecEnv, da wir nur eine Instanz brauchen)\n",
    "train_backtest_env = TradingEnv(\n",
    "    data=train_data,  # Oder test_data, falls du separate Test-Daten hast\n",
    "    initial_cash=10_000,\n",
    "    window_size=336,\n",
    "    scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = train_backtest_env.reset(seed=SEED)\n",
    "done = False\n",
    "\n",
    "# Liste der actionen\n",
    "action_list = []\n",
    "\n",
    "while not done:\n",
    "#for i in range(1000):\n",
    "    # Bestimme die Aktion (deterministisch)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)  # oder: action = action.item()\n",
    "    obs, reward, done, truncated, info = train_backtest_env.step(action)\n",
    "    action_list.append(action)\n",
    "\n",
    "# Hier wird der Zustand gerendert (z.B. als Plot). Du kannst den Render-Modus anpassen.\n",
    "train_backtest_env.render(mode='human')\n",
    "print(action_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = test_env.reset(seed=SEED)\n",
    "done = False\n",
    "\n",
    "# Liste der actionen\n",
    "action_list = []\n",
    "\n",
    "while not done:\n",
    "#for i in range(10):\n",
    "    # Bestimme die Aktion (deterministisch)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)  # oder: action = action.item()\n",
    "    obs, reward, done, truncated, info = test_env.step(action)\n",
    "    action_list.append(action)\n",
    "\n",
    "# Hier wird der Zustand gerendert (z.B. als Plot). Du kannst den Render-Modus anpassen.\n",
    "test_env.render(mode='human')\n",
    "print(action_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Trainingsschleife in Intervallen\n",
    "for i in range(1, n_iterations + 1):\n",
    "    print(f\"\\n=== Trainingsiteration {i} von {n_iterations} ===\")\n",
    "    print(\"Modell l√§uft auf:\", model.device)\n",
    "\n",
    "    # Debug-Print, um den Typ von model.policy zu pr√ºfen\n",
    "    print(f\"Typ von model.policy: {type(model.policy)}\")\n",
    "\n",
    "    # Modell auf GPU setzen, ohne model.policy zu ersetzen\n",
    "    model.policy.to(\"cuda\")\n",
    "    # Nur die forward()-Methode kompilieren, nicht das gesamte Policy-Objekt √ºberschreiben\n",
    "    model.policy.forward = torch.compile(model.policy.forward)\n",
    "\n",
    "    # Pr√ºfen, ob das Modell wirklich auf CUDA ist (Policy-Parameter verwenden)\n",
    "    print(\"Modell auf Ger√§t:\", next(model.policy.parameters()).device)\n",
    "\n",
    "    # Teste, ob Stable-Baselines3 wirklich Tensoren auf der GPU erstellt\n",
    "    test_tensor = torch.randn(10, 10).to(\"cuda\")\n",
    "    print(\"GPU-Test-Tensor erstellt:\", test_tensor.device)\n",
    "\n",
    "    # Training um 'eval_interval' Timesteps\n",
    "    print(\"Training beginnt.\")\n",
    "    start_time = time.time()\n",
    "    model.learn(total_timesteps=eval_interval, reset_num_timesteps=False)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration_minutes = duration / 60.0\n",
    "    print(f\"Training abgeschlossen. {i * eval_interval} Timesteps haben {duration:.2f} Sekunden ({duration_minutes:.2f} Minuten) gedauert.\")\n",
    "\n",
    "    # Evaluation auf dem Trainings-Environment (mittlere Reward √ºber 5 Episoden)\n",
    "    with torch.no_grad():\n",
    "        # Stelle sicher, dass die Policy auf GPU ist\n",
    "        model.policy.to(\"cuda\")\n",
    "        mean_train_reward, _ = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True, render=False)\n",
    "\n",
    "    # Evaluation auf dem Test-Environment\n",
    "    with torch.no_grad():\n",
    "        model.policy.to(\"cuda\")\n",
    "        mean_test_reward, _ = evaluate_policy(model, test_env, n_eval_episodes=5, deterministic=True, render=False)\n",
    "    \n",
    "    # Um den \"Loss\" zu erhalten, verwenden wir den negativen Reward.\n",
    "    train_loss = -mean_train_reward\n",
    "    test_loss = -mean_test_reward\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    timesteps_list.append(i * eval_interval)\n",
    "    \n",
    "    print(f\"Timesteps: {i * eval_interval} | Train Reward: {mean_train_reward:.2f} (Loss: {train_loss:.2f}) | Test Reward: {mean_test_reward:.2f} (Loss: {test_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plot: Loss-Kurven (gr√ºn: Training, rot: Test)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(timesteps_list, train_loss_list, color='green', label='Train Loss')\n",
    "plt.plot(timesteps_list, test_loss_list, color='red', label='Test Loss')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Loss (negativer Reward)\")\n",
    "plt.title(\"Train vs. Test Loss Kurven\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Trainiertes Modell speichern\n",
    "# -------------------------------\n",
    "model.save(\"ppo_trading_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
