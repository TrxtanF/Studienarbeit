{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Environment/environment_withPortfolio.ipynb\n",
    "%run ../../Environment/environment_withoutPortfolio.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardbibliotheken\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Wissenschaftliche Bibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing & Modellpersistenz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# PyTorch (für benutzerdefinierte Netzwerke)\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Reinforcement Learning (Stable Baselines 3)\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Gym Umgebung\n",
    "import gym\n",
    "\n",
    "# Hyperparameter-Tuning\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Daten setzten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "SEED  = seed % (2**32 - 1)\n",
    "print(f\"SEED: {SEED}\")\n",
    "\n",
    "INITIAL_CASH = 1\n",
    "\n",
    "WINDOW_SIZE = 336\n",
    "\n",
    "SCALER_PATH = \"../../Transform_data/scaler.pkl\"\n",
    "\n",
    "#TradingEnv = TradingEnv_withPortfolio\n",
    "TradingEnv = TradingEnv_withoutPortfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# CSV Datem einlesen\n",
    "# -------------------------------\n",
    "train_data = pd.read_csv(\"../../Transform_data/stand_data/2023-2018_stand_data.csv\")\n",
    "train_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "test_data = pd.read_csv(\"../../Transform_data/stand_data/2025-2024_stand_data.csv\")\n",
    "test_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "if(train_data is not None and test_data is not None):\n",
    "    print(\"Daten erfolgreich eingelesen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parallele Umgebungen erstellen für das Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "def create_env():\n",
    "    return TradingEnv(\n",
    "        data=train_data,\n",
    "        initial_cash=INITIAL_CASH,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        scaler_path=SCALER_PATH,\n",
    "        default_seed=SEED\n",
    "    )\n",
    "\n",
    "n_envs = 4  # Mehr parallele Umgebungen (8, 16 oder sogar 32 testen!)\n",
    "env = SubprocVecEnv([create_env for _ in range(n_envs)])\n",
    "\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "env.training = True  # Sicherstellen, dass Normalisierung aktiv ist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperparameter Evaluierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Hyperparameter-Tuning nutzen wir test_data als Validierungsdatensatz.\n",
    "valid_data = test_data.copy()\n",
    "\n",
    "# -------------------------------\n",
    "# Environment-Erstellung\n",
    "# -------------------------------\n",
    "def make_env(data):\n",
    "    def _init():\n",
    "        return TradingEnv(\n",
    "            data=data,\n",
    "            initial_cash=INITIAL_CASH,\n",
    "            window_size=WINDOW_SIZE,\n",
    "            scaler_path=SCALER_PATH,\n",
    "            default_seed=SEED\n",
    "        )\n",
    "    return _init\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation Helper Function\n",
    "# -------------------------------\n",
    "def evaluate_agent(model, env, n_eval_episodes=5):\n",
    "    episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        episode_rewards.append(total_reward)\n",
    "    return np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Hyperparameter Tuning with Optuna\n",
    "# -------------------------------\n",
    "def objective(trial):\n",
    "    # Hyperparameter-Sampling\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.90, 0.9999)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [128, 256, 512])  # keine 1024\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 1e-6, 0.01, log=True)\n",
    "    clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.4)\n",
    "    gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 0.99)\n",
    "\n",
    "    # Train-Environment mit SubprocVecEnv und VecNormalize\n",
    "    n_envs = 4\n",
    "    env_train_raw = SubprocVecEnv([make_env(train_data) for _ in range(n_envs)])\n",
    "    env_train = VecNormalize(env_train_raw, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "    env_train.training = True\n",
    "\n",
    "    # Validation-Environment mit DummyVecEnv (nur 1 Env)\n",
    "    env_valid_raw = DummyVecEnv([make_env(valid_data)])\n",
    "    env_valid = VecNormalize(env_valid_raw, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "    env_valid.training = False\n",
    "    env_valid.norm_reward = False\n",
    "\n",
    "    # PPO Agent\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env_train,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        batch_size=batch_size,\n",
    "        n_steps=n_steps,\n",
    "        ent_coef=ent_coef,\n",
    "        clip_range=clip_range,\n",
    "        gae_lambda=gae_lambda,\n",
    "        verbose=0,\n",
    "        seed=SEED,\n",
    "        policy_kwargs=dict(\n",
    "            net_arch=dict(pi=[128, 128], vf=[128, 128]),\n",
    "            activation_fn=nn.ReLU,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.learn(total_timesteps=10_000, log_interval=1)\n",
    "\n",
    "    # Evaluation\n",
    "    mean_reward = evaluate_agent(model, env_valid, n_eval_episodes=5)\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Optuna-Optimierung starten\n",
    "# -------------------------------\n",
    "def run_optuna():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "\n",
    "#run_optuna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'learning_rate': 0.004230571749056885, 'gamma': 0.9570686121852459, 'batch_size': 64, 'n_steps': 256, 'ent_coef': 2.143685006303078e-06, 'clip_range': 0.2700826948221078, 'gae_lambda': 0.8247379749162164}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Erstellen des Agenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle den PPO-Agenten ohne Hyperparametern\n",
    "model_without = PPO(\"MlpPolicy\", env, seed=SEED, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mittleres Neuronales Netz\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128, 128], vf=[128, 128])],  # Zwei Layer mit 128 Neuronen\n",
    "    activation_fn=nn.ReLU,  # Verwende ReLU als Aktivierungsfunktion\n",
    ")\n",
    "\n",
    "# Erstelle den PPO-Agenten mit den besten Optuna-Hyperparametern\n",
    "model_optuna = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.004230571749056885,\n",
    "    gamma=0.9570686121852459,\n",
    "    batch_size=64,\n",
    "    n_steps=256,\n",
    "    ent_coef=2.143685006303078e-06,\n",
    "    clip_range=0.2700826948221078,\n",
    "    gae_lambda=0.8247379749162164,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle den PPO-Agenten mit einem kleineren Netzwerk und Custom parametern\n",
    "policy_kwargs_small = dict(\n",
    "    net_arch=[dict(pi=[64, 64], vf=[64, 64])],         # Einfaches Netz\n",
    "    activation_fn=nn.Tanh,     # Testweise Tanh statt ReLU\n",
    ")\n",
    "\n",
    "model_custom_small = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.0003,         # Standard bei SB3\n",
    "    gamma=0.95,                   # Etwas kürzerer Zeithorizont\n",
    "    batch_size=64,\n",
    "    n_steps=512,                  # Etwas mehr Kontext\n",
    "    ent_coef=0.01,                # Höhere Entropiestrafe → mehr Exploration\n",
    "    clip_range=0.2,\n",
    "    gae_lambda=0.92,\n",
    "    policy_kwargs=policy_kwargs_small,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle den PPO-Agenten mit einem größeren Netzwerk und Custom parametern\n",
    "policy_kwargs_deep = dict(\n",
    "    net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])],  # Tieferes Netz\n",
    "    activation_fn=nn.ReLU,\n",
    ")\n",
    "\n",
    "model_custom_deep = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.0001,        # Sehr vorsichtige Lernrate\n",
    "    gamma=0.99,\n",
    "    batch_size=128,\n",
    "    n_steps=1024,                # Längere Rollouts\n",
    "    ent_coef=0.0001,             # Wenig Exploration\n",
    "    clip_range=0.25,\n",
    "    gae_lambda=0.95,\n",
    "    policy_kwargs=policy_kwargs_deep,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Modell trainieren und speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without.learn(total_timesteps=100_000)\n",
    "model_without.save(\"Dueck_Without_1_EUR_200K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optuna.learn(total_timesteps=100_000)\n",
    "model_optuna.save(\"Dueck_Without_Optuna_1_EUR_200K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom_small.learn(total_timesteps=100_000)\n",
    "model_custom_small.save(\"Dueck_Without_Custom_Small_1_EUR_200K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom_deep.learn(total_timesteps=100_000)\n",
    "model_custom_deep.save(\"Dueck_Without_Custom_Deep_1_EUR_200K\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
