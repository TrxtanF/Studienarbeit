{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Liste der ben√∂tigten Bibliotheken\n",
    "required_packages = [\n",
    "    \"gymnasium\", \"numpy\", \"pandas\", \"joblib\", \"scikit-learn\", \"matplotlib\",\n",
    "    \"stable-baselines3\", \"torch\", \"torchvision\", \"torchaudio\"\n",
    "]\n",
    "\n",
    "# Funktion zum Installieren fehlender Bibliotheken\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            print(f\"üì¶ Installiere {package} ...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Installiere fehlende Pakete\n",
    "install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Gibt den aktuellen Arbeitsordner aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ausgef√ºhrt\n"
     ]
    }
   ],
   "source": [
    "%run /home/dhbw/environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import random\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 42\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "SEED  = seed % (2**32 - 1)\n",
    "print(f\"SEED: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich eingelesen\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CSV Datem einlesen\n",
    "# -------------------------------\n",
    "train_data = pd.read_csv(\"/home/dhbw/2023-2018_stand_data.csv\")\n",
    "train_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "test_data = pd.read_csv(\"/home/dhbw/2025-2024_stand_data.csv\")\n",
    "test_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "if(train_data is not None and test_data is not None):\n",
    "    print(\"Daten erfolgreich eingelesen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "def make_env():\n",
    "    return TradingEnv(\n",
    "        data=train_data,\n",
    "        initial_cash=10_000,\n",
    "        window_size=336,\n",
    "        scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "        default_seed=SEED\n",
    "    )\n",
    "\n",
    "n_envs = 8  # Mehr parallele Umgebungen (8, 16 oder sogar 32 testen!)\n",
    "env = SubprocVecEnv([make_env for _ in range(n_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed in the environment: 42\n"
     ]
    }
   ],
   "source": [
    "# Test-Umgebung f√ºr Evaluation (ohne SubprocVecEnv, da wir nur eine Instanz brauchen)\n",
    "test_env = TradingEnv(\n",
    "    data=test_data,  # Oder test_data, falls du separate Test-Daten hast\n",
    "    initial_cash=10_000,\n",
    "    window_size=336,\n",
    "    scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Pr√ºfen, ob GPU Tensoren richtig verarbeitet\n",
    "tensor = torch.rand(1000, 1000).to(\"cuda\")\n",
    "for i in range(100000):\n",
    "    tensor = tensor @ tensor  # Matrix-Multiplikation, sollte die GPU stark belasten\n",
    "\n",
    "print(\"Fertig!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "env.training = True  # Sicherstellen, dass Normalisierung aktiv ist\n",
    "env.device = \"cuda\"  # WICHTIG: Umgebung auf CUDA setzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import nn  # F√ºr die Netzwerkarchitektur\n",
    "\n",
    "# Definiere das neuronale Netz\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[128, 128], vf=[128, 128])],  # Zwei Layer mit 128 Neuronen\n",
    "    activation_fn=nn.ReLU,  # Verwende ReLU als Aktivierungsfunktion\n",
    ")\n",
    "\n",
    "# Erstelle den PPO-Agenten mit verbesserten Einstellungen\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.0003,  # Lernrate\n",
    "    gamma=0.99,  # Discount-Faktor\n",
    "    clip_range=0.2,  # PPO-Clip-Parameter\n",
    "    ent_coef=0.01,  # Entropie-Koeffizient\n",
    "    n_steps=32768  ,  # WICHTIG: Mehr Schritte pro Update ‚Üí GPU-Auslastung steigt\n",
    "    batch_size=16384  ,  # WICHTIG: Gro√üe Batch-Gr√∂√üe ‚Üí GPU rechnet effizienter\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    device=\"cuda\",  # Nutzt die GPU!\n",
    "    #tensorboard_log=\"./tensorboard_log/\"  # Optional: Logging f√ºr TensorBoard\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env l√§uft auf: cuda\n",
      "Modell l√§uft auf: cuda\n",
      "Model Policy: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Env l√§uft auf: {env.device}\")  # Sollte CUDA anzeigen\n",
    "print(\"Modell l√§uft auf:\", model.device)\n",
    "print(\"Model Policy:\", model.policy.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.policies.ActorCriticPolicy'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(model.policy))\n",
    "print(hasattr(model.policy, 'to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy erfolgreich auf GPU gesetzt: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.policy.to(\"cuda\")\n",
    "print(\"Policy erfolgreich auf GPU gesetzt:\", next(model.policy.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Optional: Checkpoint Callback, falls du Zwischenspeicherungen m√∂chtest\n",
    "# checkpoint_callback = CheckpointCallback(save_freq=100, save_path='./logs/', name_prefix='ppo_trading')\n",
    "\n",
    "# -------------------------------\n",
    "# Trainings- und Test-Performance (Loss) evaluieren und plotten\n",
    "# -------------------------------\n",
    "# Wir unterteilen das Training in mehrere Intervalle.\n",
    "eval_interval = 1000          # Trainingsschritte pro Intervall\n",
    "total_timesteps = 10000       # Gesamtzahl der Trainingsschritte\n",
    "n_iterations = total_timesteps // eval_interval\n",
    "\n",
    "# Listen f√ºr Plot-Daten\n",
    "train_loss_list = []  # Wir definieren Loss als negativen Reward (damit \"kleiner\" besser ist)\n",
    "test_loss_list = []\n",
    "timesteps_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trainingsiteration 1 von 10 ===\n",
      "Modell l√§uft auf: cuda\n",
      "Typ von model.policy: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>\n",
      "Modell auf Ger√§t: cuda:0\n",
      "GPU-Test-Tensor erstellt: cuda:0\n",
      "Training beginnt.\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1454   |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 180    |\n",
      "|    total_timesteps | 262144 |\n",
      "-------------------------------\n",
      "Training abgeschlossen. 1000 Timesteps haben 201.35 Sekunden (3.36 Minuten) gedauert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Timesteps: 1000 | Train Reward: 88.88 (Loss: -88.88) | Test Reward: -3.21 (Loss: 3.21)\n",
      "\n",
      "=== Trainingsiteration 2 von 10 ===\n",
      "Modell l√§uft auf: cuda\n",
      "Typ von model.policy: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>\n",
      "Modell auf Ger√§t: cuda:0\n",
      "GPU-Test-Tensor erstellt: cuda:0\n",
      "Training beginnt.\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1476   |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 177    |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "Training abgeschlossen. 2000 Timesteps haben 198.71 Sekunden (3.31 Minuten) gedauert.\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Timesteps: 2000 | Train Reward: 265.01 (Loss: -265.01) | Test Reward: -7.87 (Loss: 7.87)\n",
      "\n",
      "=== Trainingsiteration 3 von 10 ===\n",
      "Modell l√§uft auf: cuda\n",
      "Typ von model.policy: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>\n",
      "Modell auf Ger√§t: cuda:0\n",
      "GPU-Test-Tensor erstellt: cuda:0\n",
      "Training beginnt.\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1460   |\n",
      "|    iterations      | 1      |\n",
      "|    time_elapsed    | 179    |\n",
      "|    total_timesteps | 786432 |\n",
      "-------------------------------\n",
      "Training abgeschlossen. 3000 Timesteps haben 200.76 Sekunden (3.35 Minuten) gedauert.\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Timesteps: 3000 | Train Reward: 467.12 (Loss: -467.12) | Test Reward: -3.52 (Loss: 3.52)\n",
      "\n",
      "=== Trainingsiteration 4 von 10 ===\n",
      "Modell l√§uft auf: cuda\n",
      "Typ von model.policy: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>\n",
      "Modell auf Ger√§t: cuda:0\n",
      "GPU-Test-Tensor erstellt: cuda:0\n",
      "Training beginnt.\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1473    |\n",
      "|    iterations      | 1       |\n",
      "|    time_elapsed    | 177     |\n",
      "|    total_timesteps | 1048576 |\n",
      "--------------------------------\n",
      "Training abgeschlossen. 4000 Timesteps haben 198.85 Sekunden (3.31 Minuten) gedauert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-3:\n",
      "Process ForkServerProcess-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-5:\n",
      "Process ForkServerProcess-1:\n",
      "Traceback (most recent call last):\n",
      "Process ForkServerProcess-4:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkServerProcess-6:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Process ForkServerProcess-8:\n",
      "Process ForkServerProcess-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dhbw/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed in the environment: 42\n",
      "Seed in the environment: 43\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 44\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 46\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 47\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 49\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 48\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 45\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n",
      "Seed in the environment: 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Stelle sicher, dass die Policy auf GPU ist\u001b[39;00m\n\u001b[32m     35\u001b[39m     model.policy.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     mean_train_reward, _ = \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Evaluation auf dem Test-Environment\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:94\u001b[39m, in \u001b[36mevaluate_policy\u001b[39m\u001b[34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts < episode_count_targets).any():\n\u001b[32m     88\u001b[39m     actions, states = model.predict(\n\u001b[32m     89\u001b[39m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     90\u001b[39m         state=states,\n\u001b[32m     91\u001b[39m         episode_start=episode_starts,\n\u001b[32m     92\u001b[39m         deterministic=deterministic,\n\u001b[32m     93\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     new_observations, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     current_rewards += rewards\n\u001b[32m     96\u001b[39m     current_lengths += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    202\u001b[39m \n\u001b[32m    203\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:191\u001b[39m, in \u001b[36mVecNormalize.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    189\u001b[39m             \u001b[38;5;28mself\u001b[39m.obs_rms[key].update(obs[key])\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobs_rms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.normalize_obs(obs)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/stable_baselines3/common/running_mean_std.py:36\u001b[39m, in \u001b[36mRunningMeanStd.update\u001b[39m\u001b[34m(self, arr)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, arr: np.ndarray) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     batch_mean = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     batch_var = np.var(arr, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     38\u001b[39m     batch_count = arr.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860\u001b[39m, in \u001b[36mmean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m   3857\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3860\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3861\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.12/site-packages/numpy/_core/_methods.py:135\u001b[39m, in \u001b[36m_mean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m    132\u001b[39m         dtype = mu.dtype(\u001b[33m'\u001b[39m\u001b[33mf4\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    133\u001b[39m         is_float16_result = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m ret = \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu.ndarray):\n\u001b[32m    137\u001b[39m     ret = um.true_divide(\n\u001b[32m    138\u001b[39m             ret, rcount, out=ret, casting=\u001b[33m'\u001b[39m\u001b[33munsafe\u001b[39m\u001b[33m'\u001b[39m, subok=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Trainingsschleife in Intervallen\n",
    "for i in range(1, n_iterations + 1):\n",
    "    print(f\"\\n=== Trainingsiteration {i} von {n_iterations} ===\")\n",
    "    print(\"Modell l√§uft auf:\", model.device)\n",
    "\n",
    "    # Debug-Print, um den Typ von model.policy zu pr√ºfen\n",
    "    print(f\"Typ von model.policy: {type(model.policy)}\")\n",
    "\n",
    "    # Modell auf GPU setzen, ohne model.policy zu ersetzen\n",
    "    model.policy.to(\"cuda\")\n",
    "    # Nur die forward()-Methode kompilieren, nicht das gesamte Policy-Objekt √ºberschreiben\n",
    "    model.policy.forward = torch.compile(model.policy.forward)\n",
    "\n",
    "    # Pr√ºfen, ob das Modell wirklich auf CUDA ist (Policy-Parameter verwenden)\n",
    "    print(\"Modell auf Ger√§t:\", next(model.policy.parameters()).device)\n",
    "\n",
    "    # Teste, ob Stable-Baselines3 wirklich Tensoren auf der GPU erstellt\n",
    "    test_tensor = torch.randn(10, 10).to(\"cuda\")\n",
    "    print(\"GPU-Test-Tensor erstellt:\", test_tensor.device)\n",
    "\n",
    "    # Training um 'eval_interval' Timesteps\n",
    "    print(\"Training beginnt.\")\n",
    "    start_time = time.time()\n",
    "    model.learn(total_timesteps=eval_interval, reset_num_timesteps=False)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration_minutes = duration / 60.0\n",
    "    print(f\"Training abgeschlossen. {i * eval_interval} Timesteps haben {duration:.2f} Sekunden ({duration_minutes:.2f} Minuten) gedauert.\")\n",
    "\n",
    "    # Evaluation auf dem Trainings-Environment (mittlere Reward √ºber 5 Episoden)\n",
    "    with torch.no_grad():\n",
    "        # Stelle sicher, dass die Policy auf GPU ist\n",
    "        model.policy.to(\"cuda\")\n",
    "        mean_train_reward, _ = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True, render=False)\n",
    "\n",
    "    # Evaluation auf dem Test-Environment\n",
    "    with torch.no_grad():\n",
    "        model.policy.to(\"cuda\")\n",
    "        mean_test_reward, _ = evaluate_policy(model, test_env, n_eval_episodes=5, deterministic=True, render=False)\n",
    "    \n",
    "    # Um den \"Loss\" zu erhalten, verwenden wir den negativen Reward.\n",
    "    train_loss = -mean_train_reward\n",
    "    test_loss = -mean_test_reward\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    timesteps_list.append(i * eval_interval)\n",
    "    \n",
    "    print(f\"Timesteps: {i * eval_interval} | Train Reward: {mean_train_reward:.2f} (Loss: {train_loss:.2f}) | Test Reward: {mean_test_reward:.2f} (Loss: {test_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plot: Loss-Kurven (gr√ºn: Training, rot: Test)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(timesteps_list, train_loss_list, color='green', label='Train Loss')\n",
    "plt.plot(timesteps_list, test_loss_list, color='red', label='Test Loss')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Loss (negativer Reward)\")\n",
    "plt.title(\"Train vs. Test Loss Kurven\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Trainiertes Modell speichern\n",
    "# -------------------------------\n",
    "model.save(\"ppo_trading_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# TradingEnv erstellen (bitte sicherstellen, dass TradingEnv importiert oder im gleichen Skript definiert ist)\n",
    "# -------------------------------\n",
    "env = TradingEnv(\n",
    "    data=train_data,\n",
    "    initial_cash=10_000,\n",
    "    window_size=336,\n",
    "    scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")\n",
    "\n",
    "if(env is not None):\n",
    "    print(\"Environment created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# PPO-Agenten initialisieren\n",
    "# -------------------------------\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    verbose=1, \n",
    "    seed=SEED, \n",
    "    device=\"cuda\",\n",
    "#   tensorboard_log=\"./tensorboard_log/\"\n",
    ")\n",
    "print(model.device)  # Sollte \"cuda:0\" ausgeben, wenn es auf der GPU l√§uft\n",
    "\n",
    "# Optional: Checkpoint Callback um den Trainingsfortschritt zwischendurch zu speichern\n",
    "checkpoint_callback = CheckpointCallback(save_freq=100, save_path='./logs/', name_prefix='ppo_trading')\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "model.learn(\n",
    "    total_timesteps=100, \n",
    "    #callback=checkpoint_callback, \n",
    "    log_interval=1\n",
    ")\n",
    "\n",
    "# Speichere das trainierte Modell\n",
    "model.save(\"ppo_trading_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainigsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Testlauf: Den trainierten Agenten in einer Episode ausf√ºhren\n",
    "# -------------------------------\n",
    "training_env = env\n",
    "\n",
    "obs, info = training_env.reset(seed=SEED)\n",
    "done = False\n",
    "\n",
    "# Liste der actionen\n",
    "action_list = []\n",
    "\n",
    "#while not done:\n",
    "for i in range(1000):\n",
    "    # Bestimme die Aktion (deterministisch)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)  # oder: action = action.item()\n",
    "    obs, reward, done, truncated, info = training_env.step(action)\n",
    "    action_list.append(action)\n",
    "\n",
    "# Hier wird der Zustand gerendert (z.B. als Plot). Du kannst den Render-Modus anpassen.\n",
    "training_env.render(mode='human')\n",
    "print(action_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Testlauf: Den trainierten Agenten in einer Episode ausf√ºhren\n",
    "# -------------------------------\n",
    "test_env = TradingEnv(\n",
    "    data=test_data,\n",
    "    initial_cash=10_000,\n",
    "    window_size=336,\n",
    "    scaler_path=\"../../Transform_data/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")\n",
    "\n",
    "obs, info = test_env.reset(seed=SEED)\n",
    "done = False\n",
    "\n",
    "# Liste der actionen\n",
    "action_list = []\n",
    "\n",
    "#while not done:\n",
    "for i in range(100):\n",
    "    # Bestimme die Aktion (deterministisch)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)  # oder: action = action.item()\n",
    "    obs, reward, done, truncated, info = test_env.step(action)\n",
    "    action_list.append(action)\n",
    "\n",
    "# Hier wird der Zustand gerendert (z.B. als Plot). Du kannst den Render-Modus anpassen.\n",
    "test_env.render(mode='human')\n",
    "print(action_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "#from stable_baselines3.common.callbacks import CheckpointCallback  # Optional, falls ben√∂tigt\n",
    "#from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# TradingEnv erstellen\n",
    "# (Stelle sicher, dass TradingEnv bereits importiert oder definiert ist)\n",
    "# -------------------------------\n",
    "env = TradingEnv(\n",
    "    data=train_data,\n",
    "    initial_cash=10_000,\n",
    "    window_size=336,\n",
    "    scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")\n",
    "\n",
    "test_env = TradingEnv(\n",
    "    data=test_data,\n",
    "    initial_cash=10_000,\n",
    "    window_size=336,\n",
    "    scaler_path=\"/home/dhbw/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")\n",
    "\n",
    "print(\"Environments erstellt\")\n",
    "\n",
    "# -------------------------------\n",
    "# PPO-Agent initialisieren\n",
    "# -------------------------------\n",
    "model2 = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    device=\"cuda\",\n",
    "    #tensorboard_log=\"./tensorboard_log/\"  # Optional: Logging f√ºr TensorBoard\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA verf√ºgbar:\", torch.cuda.is_available())\n",
    "print(\"Aktuelles Device:\", torch.cuda.current_device())\n",
    "print(\"Device-Name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Modell l√§uft auf:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = make_env()\n",
    "obs = env.reset()\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info, _ = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"1000 Schritte dauerten: {end_time - start_time:.2f} Sekunden\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
