{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Environment/environment_withPortfolio.ipynb\n",
    "%run ../../Environment/environment_withoutPortfolio.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "SEED  = seed % (2**32 - 1)\n",
    "print(f\"SEED: {SEED}\")\n",
    "\n",
    "INITIAL_CASH = 1\n",
    "\n",
    "WINDOW_SIZE = 336\n",
    "\n",
    "SCALER_PATH = \"../../Transform_data/scaler.pkl\"\n",
    "\n",
    "#TradingEnv = TradingEnv_withPortfolio\n",
    "TradingEnv = TradingEnv_withoutPortfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Datem einlesen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv(\"../../Transform_data/stand_data/2023-2018_stand_data.csv\")\n",
    "train_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "test_data = pd.read_csv(\"../../Transform_data/stand_data/2025-2024_stand_data.csv\")\n",
    "test_data.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "print(\"✅ Trainings- und Testdaten erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TradingEnv erstellen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv(\n",
    "    data=train_data,\n",
    "    initial_cash=INITIAL_CASH,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    scaler_path=\"../../Transform_data/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")\n",
    "\n",
    "print(\"✅ Environment erfolgreich erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Für Hyperparameter-Tuning nutzen wir test_data als Validierungsdatensatz.\n",
    "valid_data = test_data.copy()\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation Helper Function\n",
    "# -------------------------------\n",
    "def evaluate_agent(model, env, n_eval_episodes=5):\n",
    "    episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            obs, info = reset_result\n",
    "        else:\n",
    "            obs = reset_result\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            step_result = env.step(action)\n",
    "            if len(step_result) == 5:\n",
    "                obs, reward, done, truncated, info = step_result\n",
    "            else:\n",
    "                obs, reward, done, info = step_result\n",
    "            total_reward += reward\n",
    "        episode_rewards.append(total_reward)\n",
    "    return np.mean(episode_rewards)\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameter Tuning with Optuna (für A2C)\n",
    "# -------------------------------\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters for A2C\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.90, 0.9999)\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [5, 10, 16, 32, 64])\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 1e-6, 0.01, log=True)\n",
    "    vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 1.0)\n",
    "    gae_lambda = trial.suggest_float(\"gae_lambda\", 0.8, 0.99)\n",
    "\n",
    "    # Trainingsumgebung\n",
    "    env_train = DummyVecEnv([lambda: TradingEnv(\n",
    "        data=train_data,\n",
    "        initial_cash=INITIAL_CASH,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        scaler_path=\"../../Transform_data/scaler.pkl\",\n",
    "        default_seed=SEED\n",
    "    )])\n",
    "\n",
    "    # Validierungsumgebung\n",
    "    env_valid = DummyVecEnv([lambda: TradingEnv(\n",
    "        data=valid_data,\n",
    "        initial_cash=INITIAL_CASH,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        scaler_path=\"../../Transform_data/scaler.pkl\",\n",
    "        default_seed=SEED\n",
    "    )])\n",
    "\n",
    "    # A2C Modell\n",
    "    model = A2C(\n",
    "        \"MlpPolicy\",\n",
    "        env_train,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        n_steps=n_steps,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        gae_lambda=gae_lambda,\n",
    "        verbose=0,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    model.learn(total_timesteps=10000, log_interval=1)\n",
    "\n",
    "    # Evaluation\n",
    "    mean_reward = evaluate_agent(model, env_valid, n_eval_episodes=5)\n",
    "    return mean_reward\n",
    "\n",
    "# Optuna-Studie starten\n",
    "#study = optuna.create_study(direction=\"maximize\")\n",
    "#study.optimize(objective, n_trials=50)\n",
    "#print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 2  # <---- Version Auswählen\n",
    "\n",
    "\n",
    "if version == 1:\n",
    "    # Modell 1 – Standard A2C-Agent (Baseline)\n",
    "    model = A2C(\"MlpPolicy\", env, verbose=1, seed=SEED)\n",
    "    model.learn(total_timesteps=100_000)\n",
    "    model.save(\"A2C_Model1_100K\")\n",
    "\n",
    "elif version == 2:\n",
    "    # Modell 2 – Optuna-optimierte Parameter \n",
    "    model = A2C(\"MlpPolicy\", env,\n",
    "                 learning_rate=0.002181000085419467, \n",
    "                 gamma=0.9375026778731048,\n",
    "                 n_steps=5, \n",
    "                 ent_coef =4.348163518300396e-06, \n",
    "                 vf_coef=0.3865716709856257, \n",
    "                 max_grad_norm=0.5566804279536217, \n",
    "                 gae_lambda = 0.8321522058182133,\n",
    "                 verbose=1,\n",
    "                 seed=SEED)\n",
    "    model.learn(total_timesteps=100_000)\n",
    "    model.save(\"A2C_Model2_100K\")\n",
    "\n",
    "elif version == 3:\n",
    "    # Modell 3 – Agent soll mehr ausprobieren\n",
    "    model = A2C(\"MlpPolicy\",\n",
    "                env,\n",
    "                seed=SEED,\n",
    "                learning_rate=0.0007,           # moderat\n",
    "                n_steps=5,                      # kurze Entscheidungsspanne → mehr Feedback\n",
    "                gamma=0.99,\n",
    "                gae_lambda=0.95,\n",
    "                ent_coef=0.05,                  # sehr hohe Entropie → erkundet mehr\n",
    "                vf_coef=0.5,\n",
    "                max_grad_norm=0.5,\n",
    "                use_rms_prop=True,\n",
    "                normalize_advantage=True,\n",
    "                verbose=1)\n",
    "    model.learn(total_timesteps=100_000)\n",
    "    model.save(\"A2C_Model3_100K\")\n",
    "\n",
    "elif version == 4:\n",
    "    # Modell 4 – Weniger chaotisches Verhalten – Fokus auf stabile Policy-Updates.\n",
    "    model = A2C(\"MlpPolicy\",\n",
    "                env,\n",
    "                seed=SEED,\n",
    "                learning_rate=0.0001,           # deutlich niedriger\n",
    "                n_steps=20,                     # längere Rollouts\n",
    "                gamma=0.95,                     # konservativere Gewichtung zukünftiger Rewards\n",
    "                gae_lambda=0.9,\n",
    "                ent_coef=0.0001,                # fast kein Exploration-Drang\n",
    "                vf_coef=0.25,\n",
    "                max_grad_norm=0.3,\n",
    "                use_rms_prop=True,\n",
    "                normalize_advantage=True,\n",
    "                verbose=1)\n",
    "    model.learn(total_timesteps=100_000)\n",
    "    model.save(\"A2C_Model4_100K\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Ungültige Agenten-Version: nur 1 bis 4 erlaubt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainigsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_env = env\n",
    "\n",
    "obs, info = training_env.reset(seed=SEED)\n",
    "done = False\n",
    "\n",
    "# Liste der actionen\n",
    "action_list = []\n",
    "\n",
    "while not done:\n",
    "    # Bestimme die Aktion (deterministisch)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)  # oder: action = action.item()\n",
    "    obs, reward, done, truncated, info = training_env.step(action)\n",
    "    action_list.append(action)\n",
    "\n",
    "# Hier wird der Zustand gerendert (z.B. als Plot). Du kannst den Render-Modus anpassen.\n",
    "training_env.render(mode='human')\n",
    "print(action_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = TradingEnv(\n",
    "    data=test_data,\n",
    "    initial_cash=INITIAL_CASH,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    scaler_path=\"../../Transform_data/scaler.pkl\",\n",
    "    default_seed=SEED\n",
    ")\n",
    "\n",
    "obs, info = test_env.reset(seed=SEED)\n",
    "done = False\n",
    "\n",
    "# Liste der actionen\n",
    "action_list = []\n",
    "\n",
    "while not done:\n",
    "    # Bestimme die Aktion (deterministisch)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    action = int(action)  # oder: action = action.item()\n",
    "    obs, reward, done, truncated, info = test_env.step(action)\n",
    "    action_list.append(action)\n",
    "\n",
    "# Hier wird der Zustand gerendert (z.B. als Plot). Du kannst den Render-Modus anpassen.\n",
    "test_env.render(mode='human')\n",
    "print(action_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
