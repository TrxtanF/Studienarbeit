{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Bitcoin-Datenvorverarbeitung fÃ¼r Machine Learning\n",
    "\n",
    "Dieses Notebook bereitet Bitcoin-Stundendaten fÃ¼r das Training eines Machine-Learning-Modells vor. \n",
    "Es umfasst:\n",
    "- Bereinigung der Rohdaten\n",
    "- Berechnung technischer Indikatoren\n",
    "- Erzeugung relativer Features\n",
    "- Standardisierung der Daten fÃ¼r das Modell\n",
    "\n",
    "## ðŸ”§ 1. Bibliotheken importieren\n",
    "Wir laden die notwendigen Bibliotheken fÃ¼r Datenverarbeitung und technische Analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ta.trend import SMAIndicator, EMAIndicator, MACD\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands, AverageTrueRange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 2. Daten laden\n",
    "Wir importieren die Bitcoin-Stundendaten aus einer CSV-Datei. \n",
    "Die Spalten enthalten Open-High-Low-Close-Werte (OHLC), Volumendaten und Zeitstempel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"raw_data/2023-2018_BTC-USD_Data_1h.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¼ 3. Datenbereinigung\n",
    "In diesem Schritt entfernen wir nicht benÃ¶tigte Spalten, wandeln das Datum um und interpolieren fehlende Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es existieren keine fehlenden Werte.\n",
      "Es existieren keine doppelten DatensÃ¤tze.\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bereinigt die Bitcoin-Stundendaten fÃ¼r das Machine-Learning-Modell und loggt jede Ã„nderung.\n",
    "\n",
    "    Schritte:\n",
    "    1. Entfernt nicht benÃ¶tigte Spalten (`symbol`, `unix`).\n",
    "    2. Konvertiert `date` in ein datetime-Format und setzt es als Index.\n",
    "    3. ÃœberprÃ¼ft und meldet fehlende Werte, bevor sie interpoliert werden.\n",
    "    4. Entfernt Duplikate und gibt an, welche Zeilen entfernt wurden.\n",
    "    \n",
    "    :param df: Pandas DataFrame mit Bitcoin-Daten.\n",
    "    :return: Bereinigter Pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sortiere die Daten aufsteigend\n",
    "    df = df.sort_values(by='unix', ascending=True)  # Falls `unix` noch vorhanden ist\n",
    "\n",
    "    # Entferne unnÃ¶tige Spalten\n",
    "    drop_cols = ['symbol', 'unix']\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Konvertiere `date` in datetime-Format\n",
    "    df['datetime'] = pd.to_datetime(df['date'], format='%d.%m.%Y %H:%M')\n",
    "    df = df.drop(columns=['date'])\n",
    "\n",
    "    # ÃœberprÃ¼fe auf fehlende Werte und logge sie\n",
    "    missing_values = df[df.isna().any(axis=1)]\n",
    "    if not missing_values.empty:\n",
    "        print(f\"Fehlende Werte vor Interpolation:\\n{missing_values}\\n\")\n",
    "    else:\n",
    "        print(\"Es existieren keine fehlenden Werte.\")\n",
    "    \n",
    "    # Fehlende Werte mit linearer Interpolation fÃ¼llen\n",
    "    df = df.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Entferne Duplikate basierend auf dem Index (datetime)\n",
    "    duplicates = df[df.index.duplicated(keep='first')]\n",
    "    if not duplicates.empty:\n",
    "        print(f\"Entfernte Duplikate:\\n{duplicates}\\n\")\n",
    "    else:\n",
    "        print(\"Es existieren keine doppelten DatensÃ¤tze.\")\n",
    "\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_cleaned = clean_data(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ 4. Nullwerte interpolieren\n",
    "Wir ersetzen `0`-Werte in Volumendaten durch `NaN`, um eine lineare Interpolation durchfÃ¼hren zu kÃ¶nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_columns_with_zeros(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ersetzt `0`-Werte in den angegebenen Spalten durch Interpolationen.\n",
    "\n",
    "    - Konvertiert `0`-Werte in `NaN`, um sie interpolieren zu kÃ¶nnen.\n",
    "    - Nutzt lineare Interpolation, um die LÃ¼cken zu schlieÃŸen.\n",
    "\n",
    "    :param df: Pandas DataFrame mit den zu bearbeitenden Spalten.\n",
    "    :param columns: Liste der Spaltennamen, die interpoliert werden sollen.\n",
    "    :return: DataFrame mit interpolierten Werten in den angegebenen Spalten.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # Ersetze 0-Werte durch NaN, um sie interpolieren zu kÃ¶nnen\n",
    "        df[column] = df[column].replace(0, np.nan)\n",
    "        # Interpoliere die NaN-Werte (linear)\n",
    "        df[column] = df[column].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_interpolated = interpolate_columns_with_zeros(df_cleaned, columns=['Volume BTC', 'Volume USD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 5. Technische Indikatoren berechnen\n",
    "Wir berechnen verschiedene Indikatoren, die fÃ¼r das Machine Learning nÃ¼tzlich sind:\n",
    "- Gleitende Durchschnitte (SMA, EMA)\n",
    "- MACD-Indikator\n",
    "- RSI (Relative Strength Index)\n",
    "- Bollinger-BÃ¤nder\n",
    "- ATR (Average True Range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    FÃ¼gt technische Indikatoren mithilfe der `ta`-Bibliothek zu den Bitcoin-Stundendaten hinzu.\n",
    "\n",
    "    Berechnet:\n",
    "    - 10er & 50er Simple Moving Average (SMA)\n",
    "    - 10er Exponential Moving Average (EMA)\n",
    "    - MACD (12, 26, 9)\n",
    "    - Relative Strength Index (RSI) (14)\n",
    "    - Bollinger Bands (20)\n",
    "    - Average True Range (ATR) (14)\n",
    "    \n",
    "    :param df: Bereinigter Pandas DataFrame mit Bitcoin-Daten.\n",
    "    :return: Pandas DataFrame mit technischen Indikatoren.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gleitende Durchschnitte\n",
    "    df['SMA_10'] = SMAIndicator(close=df['close'], window=10).sma_indicator()\n",
    "    df['SMA_50'] = SMAIndicator(close=df['close'], window=50).sma_indicator()\n",
    "    df['EMA_10'] = EMAIndicator(close=df['close'], window=10).ema_indicator()\n",
    "\n",
    "    # MACD\n",
    "    macd = MACD(close=df['close'], window_slow=26, window_fast=12, window_sign=9)\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_Signal'] = macd.macd_signal()\n",
    "\n",
    "    # RSI\n",
    "    df['RSI_14'] = RSIIndicator(close=df['close'], window=14).rsi()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bollinger = BollingerBands(close=df['close'], window=20, window_dev=2)\n",
    "    df['Bollinger_High'] = bollinger.bollinger_hband()\n",
    "    df['Bollinger_Low'] = bollinger.bollinger_lband()\n",
    "\n",
    "    # Average True Range (ATR)\n",
    "    df['ATR_14'] = AverageTrueRange(high=df['high'], low=df['low'], close=df['close'], window=14).average_true_range()\n",
    "\n",
    "    return df\n",
    "\n",
    "df_with_indicators = add_technical_indicators(df_interpolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 6. Relative Features berechnen\n",
    "Wir berechnen relative Preisbewegungen, Trend- und VolatilitÃ¤tsmerkmale, um aussagekrÃ¤ftigere Eingaben fÃ¼r unser Modell zu generieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relative_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Berechnet relative Werte fÃ¼r bessere Modell-Performance.\n",
    "\n",
    "    - Open, High, Low relativ zum Close (`open_rel`, `high_rel`, `low_rel`)\n",
    "    - Preisbewegungen: Returns (`return_1h`, `return_24h`)\n",
    "    - Gleitende Durchschnitte relativ zum Close (`SMA_10_rel`, `SMA_50_rel`, `EMA_10_rel`)\n",
    "    - Close relativ zu SMA-Trends (`close_vs_SMA10`, `close_vs_SMA50`)\n",
    "    - MACD & Signal-Linie relativ zum Close (`MACD_rel`, `MACD_Signal_rel`)\n",
    "    - Bollinger-Band-Position als normierte Werte (`Bollinger_pct`)\n",
    "    - Candle-Shape-Indikatoren (`body_size`, `upper_shadow`, `lower_shadow`)\n",
    "    - Relative VolumenverÃ¤nderung (`vol_change_1h`, `vol_usd_change_1h`)\n",
    "\n",
    "    :param df: Pandas DataFrame mit OHLC & Indikatoren\n",
    "    :return: Pandas DataFrame mit zusÃ¤tzlichen relativen Features\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-10  # Sicherheitspuffer gegen Division durch Null\n",
    "\n",
    "    # Preisbewegungen (Returns)\n",
    "    df['return_1h'] = df['close'].pct_change(periods=1)\n",
    "    df['return_24h'] = df['close'].pct_change(periods=24)\n",
    "\n",
    "    # Relative Open, High, Low zum Close\n",
    "    df['open_rel'] = df['open'] / df['close'] - 1\n",
    "    df['high_rel'] = df['high'] / df['close'] - 1\n",
    "    df['low_rel'] = df['low'] / df['close'] - 1\n",
    "\n",
    "    # Relative Moving Averages\n",
    "    df['SMA_10_rel'] = df['SMA_10'] / df['close'] - 1\n",
    "    df['SMA_50_rel'] = df['SMA_50'] / df['close'] - 1\n",
    "    df['EMA_10_rel'] = df['EMA_10'] / df['close'] - 1\n",
    "\n",
    "    # Close relativ zu SMA-Trends (Trendrichtung)\n",
    "    df['close_vs_SMA10'] = (df['close'] - df['SMA_10']) / df['SMA_10']\n",
    "    df['close_vs_SMA50'] = (df['close'] - df['SMA_50']) / df['SMA_50']\n",
    "\n",
    "    # MACD & Signal relativ zum Close\n",
    "    df['MACD_rel'] = df['MACD'] / df['close']\n",
    "    df['MACD_Signal_rel'] = df['MACD_Signal'] / df['close']\n",
    "\n",
    "    # Bollinger Bands normiert (zwischen 0 und 1)\n",
    "    df['Bollinger_pct'] = (df['close'] - df['Bollinger_Low']) / (df['Bollinger_High'] - df['Bollinger_Low'] + epsilon)\n",
    "\n",
    "    # Candle-Shape-Indikatoren\n",
    "    df['body_size'] = (df['close'] - df['open']) / (df['high'] - df['low'] + epsilon)\n",
    "    df['upper_shadow'] = (df['high'] - np.maximum(df['open'], df['close'])) / (df['high'] - df['low'] + epsilon)\n",
    "    df['lower_shadow'] = (np.minimum(df['open'], df['close']) - df['low']) / (df['high'] - df['low'] + epsilon)\n",
    "\n",
    "    # Relative VolumenverÃ¤nderung\n",
    "    df['vol_change_1h'] = df['Volume BTC'].pct_change(periods=1)\n",
    "    df['vol_usd_change_1h'] = df['Volume USD'].pct_change(periods=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_with_relative_features = add_relative_features(df_with_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 7. Feature-Selektion\n",
    "Nicht relevante Spalten werden entfernt und die verbleibenden Features in eine logische Reihenfolge gebracht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_order_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Entfernt Ã¼berflÃ¼ssige Spalten und bringt die relevanten Features in eine logische Reihenfolge.\n",
    "\n",
    "    :param df: Pandas DataFrame mit allen Features.\n",
    "    :return: Pandas DataFrame mit den wichtigsten Features in optimierter Reihenfolge.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Liste der Features, die behalten werden sollen (in sinnvoller Reihenfolge)\n",
    "    feature_order = [\n",
    "        \"datetime\",\n",
    "        \n",
    "        # Momentum & VolatilitÃ¤t\n",
    "        \"return_1h\", \"return_24h\", \"RSI_14\", \"ATR_14\",\n",
    "        \n",
    "        # Relative OHLC-Werte\n",
    "        \"open_rel\", \"high_rel\", \"low_rel\",\n",
    "        \n",
    "        # Trend-Indikatoren\n",
    "        \"SMA_10_rel\", \"SMA_50_rel\", \"EMA_10_rel\", \n",
    "        \"close_vs_SMA10\", \"close_vs_SMA50\",\n",
    "        \n",
    "        # Momentum-Indikatoren\n",
    "        \"MACD_rel\", \"MACD_Signal_rel\",\n",
    "        \n",
    "        # VolatilitÃ¤ts-Indikatoren\n",
    "        \"Bollinger_pct\",\n",
    "        \n",
    "        # Candlestick-Formationen\n",
    "        \"body_size\", \"upper_shadow\", \"lower_shadow\",\n",
    "        \n",
    "        # Volumen-Daten\n",
    "        \"vol_change_1h\", \"vol_usd_change_1h\"\n",
    "    ]\n",
    "    \n",
    "    # Entferne alle Spalten, die nicht in feature_order sind\n",
    "    df_filtered = df[feature_order]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "df_final = filter_and_order_features(df_with_relative_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ•µï¸â€â™‚ï¸ 8. ÃœberprÃ¼fung auf NaN- oder Inf-Werte\n",
    "Wir speichern Zeilen mit fehlenden oder unendlichen Werten in einer Datei zur Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VollstÃ¤ndiger NaN/inf-Report gespeichert in: train_nan_inf_report.txt\n"
     ]
    }
   ],
   "source": [
    "def check_nan_and_inf_rows(df: pd.DataFrame, filename=\"nan_inf_report.txt\"):\n",
    "    \"\"\"\n",
    "    Speichert alle Zeilen mit NaN- oder inf-Werten in eine Textdatei, um zu Ã¼berprÃ¼fen,\n",
    "    ob diese Werte nur am Anfang oder auch mitten in den Daten auftreten.\n",
    "\n",
    "    - Speichert die Anzahl der betroffenen Zeilen.\n",
    "    - Speichert ALLE betroffenen Zeilen (mit NaN oder inf) in eine Datei.\n",
    "    \n",
    "    :param df: Pandas DataFrame.\n",
    "    :param filename: Name der Textdatei zum Speichern der Ergebnisse.\n",
    "    \"\"\"\n",
    "    # Finde alle Zeilen mit NaN oder inf-Werten\n",
    "    affected_rows = df[(df.isna().any(axis=1)) | (df.isin([np.inf, -np.inf]).any(axis=1))]\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        if affected_rows.empty:\n",
    "            file.write(\"âœ… Keine NaN- oder inf-Werte im DataFrame.\\n\")\n",
    "        else:\n",
    "            file.write(f\"âš ï¸ {len(affected_rows)} Zeilen enthalten NaN- oder inf-Werte.\\n\\n\")\n",
    "            file.write(affected_rows.to_string())  # Speichert ALLE betroffenen Zeilen\n",
    "\n",
    "    print(f\"âœ… VollstÃ¤ndiger NaN/inf-Report gespeichert in: {filename}\")\n",
    "\n",
    "check_nan_and_inf_rows(df_final, filename=\"train_nan_inf_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ 9. Entfernen der ersten 49 Zeilen\n",
    "Wir lÃ¶schen die ersten 49 Zeilen, da sie fehleranfÃ¤llig sein kÃ¶nnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_first_rows(df: pd.DataFrame, num_rows: int = 49) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    LÃ¶scht die ersten `num_rows` Zeilen eines DataFrames.\n",
    "\n",
    "    :param df: Pandas DataFrame.\n",
    "    :param num_rows: Anzahl der zu lÃ¶schenden Zeilen (Standard: 49).\n",
    "    :return: DataFrame ohne die ersten `num_rows` Zeilen.\n",
    "    \"\"\"\n",
    "    return df.iloc[num_rows:].reset_index(drop=True)\n",
    "\n",
    "df_final = drop_first_rows(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ 10. Standardisierung der Features\n",
    "Die Daten werden skaliert (Mittelwert = 0, Standardabweichung = 1), und der Skaler wird gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Skalierungsparameter gespeichert unter: scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "def standardize_features_with_datetime(df: pd.DataFrame, scaler_path: str = \"scaler.pkl\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardisiert numerische Features mit Z-Standardisierung (Mittelwert = 0, Std = 1),\n",
    "    behÃ¤lt aber die 'datetime'-Spalte unverÃ¤ndert.\n",
    "    Speichert den Skaler fÃ¼r zukÃ¼nftige Verwendung auf Test- oder Live-Daten.\n",
    "\n",
    "    :param df: Pandas DataFrame mit den zu standardisierenden Spalten.\n",
    "    :param scaler_path: Dateipfad zum Speichern des Scalers (Standard: \"scaler.pkl\").\n",
    "    :return: DataFrame mit standardisierten Features und der originalen 'datetime'-Spalte.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1ï¸âƒ£ Sicherstellen, dass 'datetime' als Spalte erhalten bleibt\n",
    "    datetime_col = df[['datetime']] if 'datetime' in df.columns else None\n",
    "\n",
    "    # 2ï¸âƒ£ Versuche, alle anderen Spalten numerisch zu konvertieren\n",
    "    df_numeric = df.drop(columns=['datetime'], errors='ignore').apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # 3ï¸âƒ£ Sicherstellen, dass nur numerische Spalten standardisiert werden\n",
    "    feature_cols = df_numeric.columns.tolist()\n",
    "    \n",
    "    # 4ï¸âƒ£ StandardScaler initialisieren und Standardisierung durchfÃ¼hren\n",
    "    scaler = StandardScaler()\n",
    "    df_numeric[feature_cols] = scaler.fit_transform(df_numeric[feature_cols])\n",
    "\n",
    "    # 5ï¸âƒ£ Skaler speichern, damit er auf Test- und Live-Daten angewendet werden kann\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"âœ… Skalierungsparameter gespeichert unter: {scaler_path}\")\n",
    "\n",
    "    # 6ï¸âƒ£ Falls 'datetime' vorhanden war, wieder hinzufÃ¼gen\n",
    "    if datetime_col is not None:\n",
    "        df_numeric.insert(0, 'datetime', datetime_col)\n",
    "\n",
    "    return df_numeric\n",
    "\n",
    "df_train = standardize_features_with_datetime(df_final)\n",
    "df_train.to_csv(\"stand_data/2023-2018_stand_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ 10.1 Gespeicherten Skaler auf Testdaten anwenden\n",
    "Damit die Testdaten mit den gleichen Skalierungsparametern transformiert werden, \n",
    "laden wir den gespeicherten Skaler und wenden ihn auf die Testdaten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_saved_scaler_with_datetime(df: pd.DataFrame, scaler_path: str = \"scaler.pkl\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Wendet einen gespeicherten Skaler auf neue Daten an (z. B. fÃ¼r Test- oder Live-Daten),\n",
    "    behÃ¤lt aber die 'datetime'-Spalte unverÃ¤ndert.\n",
    "\n",
    "    :param df: Pandas DataFrame mit den zu transformierenden Spalten.\n",
    "    :param scaler_path: Dateipfad des gespeicherten Scalers.\n",
    "    :return: DataFrame mit transformierten Features und der originalen 'datetime'-Spalte.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1ï¸âƒ£ Sicherstellen, dass 'datetime' als Spalte bleibt\n",
    "    datetime_col = df[['datetime']] if 'datetime' in df.columns else None\n",
    "\n",
    "    # 2ï¸âƒ£ Skaler laden\n",
    "    scaler = joblib.load(scaler_path)\n",
    "\n",
    "    # 3ï¸âƒ£ Nur numerische Spalten auswÃ¤hlen und standardisieren\n",
    "    df_numeric = df.drop(columns=['datetime'], errors='ignore').apply(pd.to_numeric, errors='coerce')\n",
    "    df_numeric[df_numeric.columns] = scaler.transform(df_numeric[df_numeric.columns])\n",
    "\n",
    "    # 4ï¸âƒ£ Falls 'datetime' vorhanden war, wieder hinzufÃ¼gen\n",
    "    if datetime_col is not None:\n",
    "        df_numeric.insert(0, 'datetime', datetime_col)\n",
    "\n",
    "    return df_numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª 11. Vorbereitung der Testdaten\n",
    "Die letzten 49 Zeilen der Trainingsdaten werden vor die Testdaten gesetzt, um Indikatoren korrekt zu berechnen. AnschlieÃŸend erfolgt die Standardisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es existieren keine fehlenden Werte.\n",
      "Es existieren keine doppelten DatensÃ¤tze.\n",
      "âœ… VollstÃ¤ndiger NaN/inf-Report gespeichert in: test_nan_inf_report.txt\n"
     ]
    }
   ],
   "source": [
    "def prepare_test_data_with_train_file(test_df: pd.DataFrame, train_df: pd.DataFrame, scaler_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bereitet die Testdaten vor, indem die letzten 49 Zeilen des Trainingsdatensatzes vor den Testdaten hinzugefÃ¼gt werden.\n",
    "    FÃ¼hrt alle Bereinigungen und Transformationen nach dem ZusammenfÃ¼gen durch.\n",
    "\n",
    "    Schritte:\n",
    "    1. Sortiert die Trainings- und Testdaten zeitlich aufsteigend.\n",
    "    2. Extrahiert automatisch die letzten 49 Zeilen aus den Trainingsdaten.\n",
    "    3. FÃ¼gt die Trainingszeilen vor den Testdaten hinzu.\n",
    "    4. Bereinigt und transformiert die zusammengefÃ¼gten Daten.\n",
    "    5. Entfernt die zusÃ¤tzlichen 49 Zeilen wieder.\n",
    "    6. Standardisiert die Testdaten mit der gespeicherten Skala.\n",
    "\n",
    "    :param test_df: Pandas DataFrame mit Testdaten (Rohdaten).\n",
    "    :param train_df: Pandas DataFrame mit Trainingsdaten (Rohdaten).\n",
    "    :param scaler_path: Pfad zum gespeicherten Skaler.\n",
    "    :return: Transformierter und bereinigter Testdatensatz.\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£ Sortiere die Trainings- und Testdaten aufsteigend nach 'unix'\n",
    "    train_df = train_df.sort_values(by=\"unix\").reset_index(drop=True)\n",
    "    test_df = test_df.sort_values(by=\"unix\").reset_index(drop=True)\n",
    "\n",
    "    # 2ï¸âƒ£ Extrahiere die letzten 49 Zeilen aus den Trainingsdaten\n",
    "    train_tail = train_df.tail(49)\n",
    "\n",
    "    # 3ï¸âƒ£ FÃ¼ge die Trainingszeilen vor den Testdaten hinzu\n",
    "    combined_df = pd.concat([train_tail, test_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    # 4ï¸âƒ£ Bereinige und transformiere die zusammengefÃ¼gten Daten\n",
    "    combined_df = clean_data(combined_df)  # Konvertiert auch `date` zu `datetime`\n",
    "    combined_df = interpolate_columns_with_zeros(combined_df, columns=['Volume BTC', 'Volume USD'])\n",
    "    combined_df = add_technical_indicators(combined_df)\n",
    "    combined_df = add_relative_features(combined_df)\n",
    "    combined_df = filter_and_order_features(combined_df)\n",
    "    check_nan_and_inf_rows(combined_df, filename=\"test_nan_inf_report.txt\")\n",
    "\n",
    "    # 5ï¸âƒ£ Entferne die ersten 49 Zeilen (ursprÃ¼ngliche Trainingsdaten)\n",
    "    combined_df = drop_first_rows(combined_df, num_rows=len(train_tail))\n",
    "\n",
    "    # 6ï¸âƒ£ Wende den gespeicherten Skaler auf die Testdaten an\n",
    "    combined_df = apply_saved_scaler_with_datetime(combined_df, scaler_path)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "test_df_raw = pd.read_csv(\"raw_data/2025-2024_BTC-USD_Data_1h.csv\")\n",
    "df_test_prepared = prepare_test_data_with_train_file(test_df_raw, df, scaler_path=\"scaler.pkl\")\n",
    "df_test_prepared.to_csv(\"stand_data/2025-2024_stand_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stand_data/2025-2024_stand_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Beispielaufruf:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m convert_us_csv_to_de_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstand_data/2023-2018_stand_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde-Format_2023-2018_stand_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mconvert_us_csv_to_de_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstand_data/2025-2024_stand_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mde-Format_2025-2024_stand_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[97], line 3\u001b[0m, in \u001b[0;36mconvert_us_csv_to_de_csv\u001b[1;34m(input_csv, output_csv)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_us_csv_to_de_csv\u001b[39m(input_csv, output_csv):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Lese die US-CSV ein (Standard-Trennzeichen: Komma)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Schreibe die CSV im deutschen Format: \u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# - Separator: Semikolon\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# - Dezimaltrennzeichen: Komma\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(output_csv, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tlfin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlfin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\tlfin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlfin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\tlfin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stand_data/2025-2024_stand_data.csv'"
     ]
    }
   ],
   "source": [
    "def convert_us_csv_to_de_csv(input_csv, output_csv):\n",
    "    # Lese die US-CSV ein (Standard-Trennzeichen: Komma)\n",
    "    df = pd.read_csv(input_csv, delimiter=',')\n",
    "    \n",
    "    # Schreibe die CSV im deutschen Format: \n",
    "    # - Separator: Semikolon\n",
    "    # - Dezimaltrennzeichen: Komma\n",
    "    df.to_csv(output_csv, sep=';', index=False, decimal=',')\n",
    "\n",
    "# Beispielaufruf:\n",
    "convert_us_csv_to_de_csv('stand_data/2023-2018_stand_data.csv', 'stand_data/de-Format_2023-2018_stand_data.csv')\n",
    "convert_us_csv_to_de_csv('stand_data/2025-2024_stand_data.csv', 'stand_data/de-Format_2025-2024_stand_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
